---
title: "Reuters newswires multi-class classification"
format: html
editor: visual
---

## Reuters newswires multi-class classification (topic)

#### Packages

```{r}
library(keras)
```

#### Load Reuters data set

```{r}
reuters <- dataset_reuters(num_words = 10000)
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% reuters
```

-   There are 8982 samples/observations in `train_data` and 2246 samples in `test_data`

-   Each sample is a list of integers (word indices)

#### Decoding newswires back to text

If we want to convert the the integers back to text, we can use the following code:

```{r, eval = FALSE}
# Convert first sample back to words
word_index <- dataset_reuters_word_index()
reverse_word_index <- names(word_index)
names(reverse_word_index) <- word_index
decode_newswire <- sapply(train_data[[1]], function(index){
  word <- if (index >= 3){
    reverse_word_index[[as.character(index-3)]]
  }
  if (!is.null(word)){
    word
  } else {
    "?"
  }
})
```

#### Preparing the data

We first need to encode/vectorize the data

```{r}
vectorize_sequences <- function(sequences, dimension = 10000){
  results <- matrix(0, nrow = length(sequences), ncol = dimension)
  for (i in 1:length(sequences)){
    results[i, sequences[[i]]] <- 1
  }
  return(results)
}

x_train <- vectorize_sequences(train_data)
x_test <- vectorize_sequences(test_data)
```

There are two ways to vectorize the labels:

-   Cast the label list as an integer tensor

-   One-hot encoding

One-hot, or [categorical encoding]{.underline} is a widely used format for categorical data. In this case, one-hot consists of embedding each label as an all-zero vector with a 1 in the pace of the label index.

##### Example: One-shot encoding

```{r, eval = FALSE}
to_one_hot <- function(labels, dimension = 46){
  results <- matrix(0, nrow = length(labels), ncol = dimension)
  for (in in 1:length(label)){
    results[i, labels[[i]] + 1] <- 1
  }
  return(results)
}

one_hot_train_labels <- to_one_hot(train_labels)
one_hot_test_labels <- to_one_hot(test_labels)
```

This one-hot encoding can also be done with Keras' built-in function:

```{r}
one_hot_train_labels <- to_categorical(train_labels)
one_hot_test_labels <- to_categorical(test_labels)
```

#### Building the network

Using 64 unit layers:

```{r}
# Model Definition
model <- keras_model_sequential() %>%
  layer_dense(units = 64,
              activation = "relu",
              input_shape = c(10000)) %>%
  layer_dense(units = 64,
              activation = "relu") %>%
  # Using softmax to output probabilty dist
  layer_dense(units = 46,
              activation = "softmax")

# Compile the model
model %>% 
  compile(optimizer = "rmsprop",
          loss = "categorical_crossentropy",
          metrics = c("accuracy"))

```

-   Using `categorical_crossentropy` to measure the difference in the distribution between the network's output and the true distribution of the labels.

### Validating the approach

To validate the model produced by the training data, we'll set aside 1000 samples for the validation set.

```{r}
val_indices <- 1:1000

x_val <- x_train[val_indices, ]
partial_x_train <- x_train[-val_indices, ]

y_val <- one_hot_train_labels[val_indices, ]
partial_y_train <- one_hot_train_labels[-val_indices, ]
```

#### Train the model

We'll start by training the network for 20 epochs

```{r}
history <- model %>%
  fit(partial_x_train,
      partial_y_train,
      epochs = 20,
      batch_size = 512,
      validation_data = list(x_val, y_val))
```

```{r}
max(history$metrics$val_accuracy)
which.max(history$metrics$val_accuracy)
```

```{r}
plot(history)
```

-   We see overfitting starting around epoch 8-9, even though the highest accuracy for the validation set is 11. Let's see how well we do for a model with just 9 epochs

#### Retraining for 9 epochs

```{r}
model_9epoch <- keras_model_sequential() %>% 
  layer_dense(units = 64,
              activation = "relu",
              input_shape = c(10000)) %>% 
  layer_dense(units = 64, 
              activation = "relu") %>%
  layer_dense(units = 46, 
                activation = "softmax") 

model_9epoch %>%
  compile(optimizer = "rmsprop",
           loss = "categorical_crossentropy",
           metrics = c("accuracy")) 

history_9epoch <- model_9epoch %>% 
  fit(partial_x_train, 
      partial_y_train, 
      epochs = 9,
      batch_size = 512, 
      validation_data = list(x_val, y_val)) 

results_9epoch <- model_9epoch %>% evaluate(x_test, one_hot_test_labels)
```

```{r}
results_9epoch
```

-   With 9 epochs, we reach an accuracy of 0.78, which is better than random chance for a binary classifier (0.50) and much better than random classifier for a multi-class (\~0.18)

    -   Got the 0.18 by taking a random sample and seeing how they scored

#### Generating predictions on new data

Let's verify that the `predict` method of the model instance returns a probability distribution over all 46 topics. We want to generate topic predictions for all of the test data:

```{r}
prediction <- model %>% predict(x_test)

# Check to make sure each entry in preidctions is a vector of length 46
dim(prediction)

# Make sure the coefficients in the vector add up to 1
sum(prediction[1, ])

# The largest entry is the predicted class, which for the first sample is:
which.max(prediction[1, ])

```

#### The importance of sufficiently large intermediate layers

Since the final outputs are 46-dimensional, we should avoid have layers with less than 46 hidden units. We can see the impact of the information bottleneck by changing the middle layer to have only 4 hidden units

```{r}
model_small_layer <- keras_model_sequential() %>% 
  layer_dense(units = 64,
              activation = "relu",
              input_shape = c(10000)) %>%
  layer_dense(units = 4,
              activation = "relu") %>%
  layer_dense(units = 46,
              activation = "softmax") 

model_small_layer %>% 
  compile(optimizer = "rmsprop",
          loss = "categorical_crossentropy",
          metrics = c("accuracy") ) 

model_small_layer %>% 
  fit(partial_x_train,
      partial_y_train,
      epochs = 20,
      batch_size = 128,
      validation_data = list(x_val, y_val))
```

-   The information bottleneck (loss due to few hidden units in comparison to the final dimensions) causes the accuracy to go down to 71%.

    -   The drop is due to network trying to compress a lot of information (information to recover the separation hyperplanes of 46 classes) into an intermediate space that has too few dimensions.

    -   The network can squeeze most of the information, into these 8D representations, but not all.

#### Impact on accuracy when increasing/decreasing hidden units

What happens if we decrease all the hidden units to 32?

```{r}
model_32unit <- keras_model_sequential() %>% 
  layer_dense(units = 32,
              activation = "relu",
              input_shape = c(10000)) %>%
  layer_dense(units = 32,
              activation = "relu") %>%
  layer_dense(units = 46,
              activation = "softmax") 

model_32unit %>% 
  compile(optimizer = "rmsprop",
          loss = "categorical_crossentropy",
          metrics = c("accuracy") ) 

model_32unit %>% 
  fit(partial_x_train,
      partial_y_train,
      epochs = 20,
      batch_size = 128,
      validation_data = list(x_val, y_val))
```

-   When we decrease the units to 32 we only see a slight drop in the accuracy (0.81 vs 0.82)

What happens when if we increase the hidden units to 128?

```{r}
model_128unit <- keras_model_sequential() %>% 
  layer_dense(units = 128,
              activation = "relu",
              input_shape = c(10000)) %>%
  layer_dense(units = 128,
              activation = "relu") %>%
  layer_dense(units = 46,
              activation = "softmax") 

model_128unit %>% 
  compile(optimizer = "rmsprop",
          loss = "categorical_crossentropy",
          metrics = c("accuracy") ) 

model_128unit %>% 
  fit(partial_x_train,
      partial_y_train,
      epochs = 20,
      batch_size = 128,
      validation_data = list(x_val, y_val))
```

-   There was no advantage to doubling the number of hidden units to 128 (0.82, 0.82)

#### Changing the number of layers

How does the accuracy change if we remove the intermediate layer?

```{r}
model_2layer <- keras_model_sequential() %>% 
  layer_dense(units = 64,
              activation = "relu",
              input_shape = c(10000)) %>%
  layer_dense(units = 46,
              activation = "softmax") 

model_2layer %>% 
  compile(optimizer = "rmsprop",
          loss = "categorical_crossentropy",
          metrics = c("accuracy") ) 

model_2layer %>% 
  fit(partial_x_train,
      partial_y_train,
      epochs = 20,
      batch_size = 128,
      validation_data = list(x_val, y_val))
```

-   We actually get better results (accuracy) with only two layers (0.833 vs 0.82)

What if we increase the number of hidden layers to 3?

```{r}
model_3layer <- keras_model_sequential() %>% 
  layer_dense(units = 64,
              activation = "relu",
              input_shape = c(10000)) %>%
  layer_dense(units = 64,
              activation = "relu") %>%
  layer_dense(units = 64,
              activation = "relu") %>%
  layer_dense(units = 46,
              activation = "softmax") 

model_3layer %>% 
  compile(optimizer = "rmsprop",
          loss = "categorical_crossentropy",
          metrics = c("accuracy") ) 

model_3layer %>% 
  fit(partial_x_train,
      partial_y_train,
      epochs = 20,
      batch_size = 128,
      validation_data = list(x_val, y_val))
```

-   We see a slight decrease in accuracy (0.81 vs 0.82) that is not really significant but we did more work to get worse results.
