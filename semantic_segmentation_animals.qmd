---
title: "Semantic image segmentation of cats and dogs"
format: html
editor: visual
---

# Semantic segmentation using images of cats and dogs

In this project, we'll be using semantic segmentation to isolate cats and dogs from their background images.

-   The dataset will be the [Oxford-IIIT Pets database](http://www.robots.ox.ac.uk/~vgg/%20data/pets/), which contains over 7,390 images of different cats and dogs.

    -   Each of these images are paired with foreground-background segmentation masks

    -   The pixel of the segmentation masks will have one of three values:

        -   Foreground: 1

        -   Background: 2

        -   Contour: 3

## Packages

```{r}
library(fs)
library(keras3)
library(tensorflow)
library(tfdatasets)
```

Wake up `reticulate`:

```{r}
op_add(1, 1)
```

## Data

First, we'll need to download and uncompressed the dataset using the `download.file()` and `untar()` utilities in R. We'll also use the `fs` package for filesystem operations.

```{r}
# Create directory
data_dir <- path("pets_dataset")
dir_create(data_dir)
```

```{r, eval = FALSE}
data_url <- path("https://thor.robots.ox.ac.uk/~vgg/data/pets")
for (filename in c("images.tar.gz", "annotations.tar.gz")) {
  download.file(url =  data_url / filename,
                destfile = data_dir / filename)
  untar(data_dir / filename, exdir = data_dir)
}

```

-   The input pictures are stored as JPG files and the corresponding segmentation masks are stored as PNG files.

Next, we'll create a tibble with columns for the input file paths and the corresponding mask file paths:

```{r}
input_dir <- data_dir / "images"
target_dir <- data_dir / "annotations/trimaps/"

image_paths <- tibble::tibble(
  # Sort list so that the input matches the target
  input = sort(dir_ls(input_dir, glob = "*.jpg")),
  target = sort(dir_ls(target_dir, glob = "*.png"))
)
```

-   `dir_ls()` is equivalent to the `ls` command and returns file names as a named `fs_path` character vector.
-   This will keep track of the paths and make sure both the input and target vectors stay in sync.

```{r, eval = FALSE}
# Check that image_paths looks right
tibble::glimpse(image_paths)
```

### View image and its mask

We can view an image and its corresponding mask using the TensorFlow API. The first step will be to create a helper function that will plot a TensorFlow Tensor containing an image using R's `plot()` function:

```{r, eval = FALSE}
display_image_tensor <- function(x, ..., max = 255, plot_margins = c(0, 0, 0, 0)){
  # If there are plot_margins
  if(!is.null(plot_margins)) {
    # Default is no margins when plotting images
    par(mar = plot_margins)
  }
  
  x %>%
    # Convert TF Tensor to R array
    as.array() %>%
    # Remove axes that are size 1
    drop() %>%
    # Convert R array into raster object
    as.raster(max = max) %>%
    plot(..., interpolate = FALSE)
}
```

-   `drop()` example: If x is a grayscale image with one color channel, it would squeeze the tensor shape from (height, width, 1) to (height, width)

-   We use `as.raster(max = max)`, which is 255 because the images are encoded as `uint8`, which only have a range from \[0, 255\]

    -   By setting `max = 255`, R will plot the 255 pixels as white and 0 as black and then interpolate linearly for values between them as shades of grey.

-   `interpolate = FALSE` tells R to draw pixels with sharp edges, no blending or interpolation of colors between pixels.

We can now read an image into a Tensor and view it:

```{r, eval = FALSE}
# View 10th image
image_tensor <- image_paths$input[10] %>%
  tf$io$read_file() %>%
  tf$io$decode_jpeg()

# Verify that the image is loaded into a Tensor
str(image_tensor)

# View image in tensor
display_image_tensor(image_tensor)
```

To see the target image, the segmentation mask, we can create another helper function:

-   Note: The target images are still `uint8` but they only have values of `(1, 2, 3)`

    -   To plot it, because this is a Tensor being converted to an R array, we need to subtract 1 in order to get it into Python's sill `(0, 1, 2)`

        -   0: Black

        -   1: Gray

        -   2: White

```{r, eval = FALSE}
display_target_tensor <- function(target){
  display_image_tensor(target - 1, max = 2)
}
```

To view the mask of the 10th image:

```{r, eval = FALSE}
target <- image_paths$target[10] |>
  tf$io$read_file() |>
  tf$io$decode_png()

# View that it is a tensor
str(target)

# View image
display_target_tensor(target)
```

### Load files into TF Datasets and split into train/validation

We'll load the input and targets into two TF Datasets and then split them into training and validation sets.

-   Since the dataset is pretty small, this can all be loaded into memory

```{r}
# Helper to read and resize images using TensorFlow operations
tf_read_image <-
  function(path, format = "image", resize = NULL, ...){
    img <- path |>
      tf$io$read_file() |>
      # Look up decode_image(), decode_jpg, or decode_png() from tf$io submodule
      tf$io[[paste0("decode_", format)]](...)
    
    if(!is.null(resize))
      img <- img |>
        # Rememver: tf module functions use integers so we need as.integer()
        tf$image$resize(as.integer(resize))
    
    return(img)
  }

# Image size will be 200 x 200
img_size <- c(200, 200)

tf_read_image_and_resize <- function(..., resize = img_size){
  tf_read_image(..., resize = resize)
}
```

```{r}
# Function to create dataset from images and store in RAM 
make_dataset <- function(paths_df){
  tensor_slices_dataset(paths_df) |>
    dataset_map(function(path){
      image <- path$input |>
        # Each input image has three channels (RGB)
        tf_read_image_and_resize("jpeg", channels = 3L)
      target <- path$target |>
        # Each target image has a single channel for each pixel
        tf_read_iamge_and_resize("png", channels = 1L)
      # Subtract 1 to get labels to (0, 1, 2)...silly Python..
      target <- target - 1
      list(image, target)
    }) |>
    dataset_cache() |>
    dataset_shuffle(buffer_size = nrow(paths_df)) |>
    dataset_batch(32)
}
```

-   The R function passed to `dataset_map()` is called with a symbolic tensor, which means it has to return one as well.

    -   `dataset_map()` takes in a single argument, a named list of two scalar string tensors. Each of these contain paths to the input and target images.

-   Caching the dataset store the full dataset in RAM after the first run.

    -   If the dataset is too big, just remove the `dataset_cache()` and the image files will loaded as needed during training.

Now we need to split the dataset into training and validation sets. We'll allocate 1000 samples for validation and the rest will be for training.

```{r}
# Reserver 1000 samples for validation
num_val_samples <- 1000
val_idx <- sample.int(nrow(image_paths), num_val_samples)

# Split into training and validation sets
val_paths <- image_paths[val_idx, ]
# Everyting not in the validation set goes into the training set
train_paths <- image_paths[-val_idx, ]

# Create datasets
validation_dataset <- make_dataset(val_paths)
train_dataset <- make_dataset(train_paths)
```

### Define the model
