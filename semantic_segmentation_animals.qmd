---
title: "Semantic image segmentation of cats and dogs"
format: html
editor: visual
---

# Semantic segmentation using images of cats and dogs

In this project, we'll be using semantic segmentation to isolate cats and dogs from their background images.

-   The dataset will be the [Oxford-IIIT Pets database](http://www.robots.ox.ac.uk/~vgg/%20data/pets/), which contains over 7,390 imafgbtg of different cats and dogs.

    -   Each of these images are paired with foreground-background segmentation masks

    -   The pixel of the segmentation masks will have one of three values:

        -   Foreground: 1

        -   Background: 2

        -   Contour: 3

## Packages

```{r}
library(fs)
library(keras3)
library(tensorflow)
library(tfdatasets)
```

Wake up `reticulate`:

```{r}
op_add(1, 1)
```

## Data

First, we'll need to download and uncompressed the dataset using the `download.file()` and `untar()` utilities in R. We'll also use the `fs` package for filesystem operations.

```{r}
# Create directory
data_dir <- path("pets_dataset")
dir_create(data_dir)
```

```{r, eval = FALSE}
options(timeout=100)
data_url <- path("https://thor.robots.ox.ac.uk/~vgg/data/pets")
for (filename in c("images.tar.gz", "annotations.tar.gz")) {
  download.file(url =  data_url / filename,
                destfile = data_dir / filename)
  untar(data_dir / filename, exdir = data_dir)
}

```

-   The input pictures are stored as JPG files and the corresponding segmentation masks are stored as PNG files.

Next, we'll create a tibble with columns for the input file paths and the corresponding mask file paths:

```{r}
input_dir <- data_dir / "images"
target_dir <- data_dir / "annotations/trimaps/"

image_paths <- tibble::tibble(
  # Sort list so that the input matches the target
  input = sort(dir_ls(input_dir, glob = "*.jpg")),
  target = sort(dir_ls(target_dir, glob = "*.png"))
)
```

-   `dir_ls()` is equivalent to the `ls` command and returns file names as a named `fs_path` character vector.
-   This will keep track of the paths and make sure both the input and target vectors stay in sync.

```{r, eval = FALSE}
# Check that image_paths looks right
tibble::glimpse(image_paths)
```

### View image and its mask

We can view an image and its corresponding mask using the TensorFlow API. The first step will be to create a helper function that will plot a TensorFlow Tensor containing an image using R's `plot()` function:

```{r}
display_image_tensor <- function(x, ..., max = 255, plot_margins = c(0, 0, 0, 0)){
  # If there are plot_margins
  if(!is.null(plot_margins)) {
    # Default is no margins when plotting images
    par(mar = plot_margins)
  }
  
  x %>%
    # Convert TF Tensor to R array
    as.array() %>%
    # Remove axes that are size 1
    drop() %>%
    # Convert R array into raster object
    as.raster(max = max) %>%
    plot(..., interpolate = FALSE)
}
```

-   `drop()` example: If x is a grayscale image with one color channel, it would squeeze the tensor shape from (height, width, 1) to (height, width)

-   We use `as.raster(max = max)`, which is 255 because the images are encoded as `uint8`, which only have a range from \[0, 255\]

    -   By setting `max = 255`, R will plot the 255 pixels as white and 0 as black and then interpolate linearly for values between them as shades of grey.

-   `interpolate = FALSE` tells R to draw pixels with sharp edges, no blending or interpolation of colors between pixels.

We can now read an image into a Tensor and view it:

```{r, eval = FALSE}
# View 10th image
image_tensor <- image_paths$input[10] %>%
  tf$io$read_file() %>%
  tf$io$decode_jpeg()

# Verify that the image is loaded into a Tensor
str(image_tensor)

# View image in tensor
display_image_tensor(image_tensor)
```

To see the target image, the segmentation mask, we can create another helper function:

-   Note: The target images are still `uint8` but they only have values of `(1, 2, 3)`

    -   To plot it, because this is a Tensor being converted to an R array, we need to subtract 1 in order to get it into Python's sill `(0, 1, 2)`

        -   0: Black

        -   1: Gray

        -   2: White

```{r}
display_target_tensor <- function(target){
  display_image_tensor(target - 1, max = 2)
}
```

To view the mask of the 10th image:

```{r, eval = FALSE}
target <- image_paths$target[10] |>
  tf$io$read_file() |>
  tf$io$decode_png()

# View that it is a tensor
str(target)

# View image
display_target_tensor(target)
```

### Load files into TF Datasets and split into train/validation

We'll load the input and targets into two TF Datasets and then split them into training and validation sets.

-   Since the dataset is pretty small, this can all be loaded into memory

```{r}
# Helper to read and resize images using TensorFlow operations
tf_read_image <-
  function(path, format = "image", resize = NULL, ...){
    img <- path |>
      tf$io$read_file() |>
      # Look up decode_image(), decode_jpg, or decode_png() from tf$io submodule
      tf$io[[paste0("decode_", format)]](...)
    
    if(!is.null(resize))
      img <- img |>
        # Rememver: tf module functions use integers so we need as.integer()
        tf$image$resize(as.integer(resize))
    
    return(img)
  }

# Image size will be 200 x 200
img_size <- c(200, 200)

tf_read_image_and_resize <- function(..., resize = img_size){
  tf_read_image(..., resize = resize)
}
```

```{r}
# Function to create dataset from images and store in RAM 
make_dataset <- function(paths_df){
  tensor_slices_dataset(paths_df) |>
    dataset_map(function(path){
      image <- path$input |>
        # Each input image has three channels (RGB)
        tf_read_image_and_resize("jpeg", channels = 3L)
      target <- path$target |>
        # Each target image has a single channel for each pixel
        tf_read_image_and_resize("png", channels = 1L)
      # Subtract 1 to get labels to (0, 1, 2)...silly Python..
      target <- target - 1
      list(image, target)
    }) |>
    dataset_cache() |>
    dataset_shuffle(buffer_size = nrow(paths_df)) |>
    dataset_batch(32)
}
```

-   The R function passed to `dataset_map()` is called with a symbolic tensor, which means it has to return one as well.

    -   `dataset_map()` takes in a single argument, a named list of two scalar string tensors. Each of these contain paths to the input and target images.

-   Caching the dataset store the full dataset in RAM after the first run.

    -   If the dataset is too big, just remove the `dataset_cache()` and the image files will loaded as needed during training.

Now we need to split the dataset into training and validation sets. We'll allocate 1000 samples for validation and the rest will be for training.

```{r}
# Reserver 1000 samples for validation
num_val_samples <- 1000
val_idx <- sample.int(nrow(image_paths), num_val_samples)

# Split into training and validation sets
val_paths <- image_paths[val_idx, ]
# Everyting not in the validation set goes into the training set
train_paths <- image_paths[-val_idx, ]

# Create datasets
validation_dataset <- make_dataset(val_paths)
train_dataset <- make_dataset(train_paths)
```

### Define the model

```{r}
get_model <- function(img_size, num_classes){
  # Local functions to avoid passing the same arguments to each call
  conv <- function(..., padding = "same", activation = "relu") {
    layer_conv_2d(..., padding = padding, activation = activation)
  }
  conv_transpose <- function(..., padding = "same", activation = "relu") {
    layer_conv_2d_transpose(..., padding = padding, activation = activation)
  }
  
  input <- layer_input(shape = c(img_size, 3))
  output <- input |>
    # Rescale input image to [0, 1] range
    layer_rescaling(scale = 1/255) |>
    conv(64, 3, strides = 2) %>%
    conv(64, 3) %>%
    conv(128, 3, strides = 2) %>%
    conv(128, 3) %>%
    conv(256, 3, strides = 2) %>%
    conv(256, 3) %>%
    conv_transpose(256, 3) %>%
    conv_transpose(256, 3, strides = 2) %>%
    conv_transpose(128, 3) %>%
    conv_transpose(128, 3, strides = 2) %>%
    conv_transpose(64, 3) %>%
    conv_transpose(64, 3, strides = 2) %>%
    conv(num_classes, 3, activation = "softmax")

  keras_model(input, output)
  
}
```

-   Use `padding = "same"` to avoid influence of the border padding on the feature map size

-   The model ends with a per-pixel 3-way softmax to classify each output pixel as one of the three categories

```{r}
model <- get_model(img_size = img_size, num_classes = 3)

model
```

-   Using strides instead of `MaxPooling2D` layers for downsampling in order to preserve spatial location information in the image

### Compile and fit the model

```{r}
# Compile the model
model %>%
    compile(optimizer = "rmsprop",
            loss = "sparse_categorical_crossentropy")

# Call back that saves the best model
callbacks <- list(
  callback_model_checkpoint("oxford_segmentation.keras",
                            save_best_only = TRUE))
# Fit the model
history <- model %>% fit(
  train_dataset,
  epochs = 25,
  callbacks = callbacks,
  validation_data = validation_dataset
)
```

-   Note: It's possible to get a warning during training to the effect of "`Corrupt JPEG data: premature end of the data segment"`. The image dataset isn't perfect, but the `tf$io` functions can usually recover gracefully.

We can display the training and validation loss of the model:

```{r}
plot(history)
```

-   I happen to know that it overfitting starts around 25 epochs, so I only went 25 to save time.

We reload the best performing model (based on validation loss) to see how it would predict a segmentation mask:

```{r}
# Load best performing model
model <- load_model("oxford_segmentation.keras")

# Select image 102 to predict its mask
test_image <- val_paths$input[102] %>%
  tf_read_image_and_resize("jpeg", channels = 3L)

predicted_mask_probs <-
  model(test_image[tf$newaxis, , , ])

# predicted_mask is a tensor with shape=(1, 200, 200), dtype = int64
predicted_mask <-
  tf$argmax(predicted_mask_probs, axis = -1L)

predicted_target <- predicted_mask + 1

par(mfrow = c(1, 2))
display_image_tensor(test_image)
display_target_tensor(predicted_target)
```

-   `tf$argmax()` is similar to `which.max()` in R.

    -   A key difference is that `tf$argmax()` returns 0-based values

### Mini Xception-like model

For this example, we'll create a smaller version of Xception and apply it to the dogs vs cats classification task that we did previously. In this example, we'll replace the model definition with the following convnet:

```{r}
data_augmentation <- keras_model_sequential() %>%
  layer_random_flip("horizontal") %>%
  layer_random_rotation(0.1) %>%
  layer_random_zoom(0.2)

inputs <- layer_input(shape = c(180, 180, 3))

x <- inputs %>%
  data_augmentation() %>%
  layer_rescaling(scale = 1 / 255)

x <- x %>%
  layer_conv_2d(32, 5, use_bias = FALSE)

for (size in c(32, 64, 128, 256, 512)) {
  residual <- x
  
  x <- x %>%
    layer_batch_normalization() %>%
    layer_activation("relu") %>%
    
    layer_separable_conv_2d(size, 3, padding = "same", use_bias = FALSE) %>%
    layer_batch_normalization() %>%
    layer_activation("relu") %>%

    layer_separable_conv_2d(size, 3, padding = "same", use_bias = FALSE) %>%
    layer_max_pooling_2d(pool_size = 3, strides = 2, padding = "same")

  residual <- residual %>%
    layer_conv_2d(size, 1, strides = 2, padding = "same", use_bias = FALSE)
  x <- layer_add(list(x, residual))
}

outputs <- x %>%
  layer_global_average_pooling_2d() %>%
  layer_dropout(0.5) %>%
  layer_dense(1, activation = "sigmoid")

model <- keras_model(inputs, outputs)

train_dataset <- image_dataset_from_directory(
  "cats_vs_dogs_small/train",
  image_size = c(180, 180),
  batch_size = 32
)

validation_dataset <- image_dataset_from_directory(
  "cats_vs_dogs_small/validation",
  image_size = c(180, 180),
  batch_size = 32
)

model %>%
  compile(
  loss = "binary_crossentropy",
  optimizer = "rmsprop",
  metrics = "accuracy"
)

history <- model %>%
  fit(train_dataset,
      epochs = 100,
      validation_data = validation_dataset)
```

```{r}
plot(history)
```

The mini-Xception model is able to achieve a test accuracy of 90.8%, whereas the naive model achieved an accuracy of 81.4%

## Interpreting what convnets learn

Deep learning model are often referred to as "black boxes" because it is hard to extract the learned representations in a form that is interpretable by humans. Convnets, on the other hand, are much easily to visual since they are representations of visual concepts. We'll use three techniques for visualizing the representations:

### Visualizing intermediate activations

Visualizing intermediate activations is done by displaying values that are returned by a models convolution and pooling layers given a specific input. We can visualize the feature maps with 3 dimensions: height, width, and depth (channels). Since each channel encodes independent features, we'll visualize the maps by independently plotting the contents of each channel as a 2D image.

To start, we will load the convnet model with augmentations:

```{r}
model <- load_model("convnet_from_scratch_with_augmentation.keras")
model
```

Next, we'll use a cat image that is not part of the dataset used to train the network:

```{r}
# Download test image
img_path <- get_file(
  fname = "cat.jpg",
  origin = "https://img-datasets.s3.amazonaws.com/cat.jpg"
)

# Read and resize image to float32. Tensor shape is (180, 180, 3)
img_tensor <- img_path |>
  tf_read_image(resize = c(180, 180))
```

We can view the downloaded cat image after being resized:

```{r}
display_image_tensor(img_tensor)
```

#### Build the model

To extract the feature maps, we need to create a Keras model that takes in batches of images as inputs and outputs the activations of the convolution and pooling layers.

We will start by creating a dummy convolution and pooling layers to determine what the S3 classname is. This is usually a long string like `keras.layers.convolutional.Conv2D,` but can change between versions of TensorFlow. It's best not to hard code the classnames

```{r}
conv_layer_s3_classname <-
  # The 1 returns the primary? class type. The one that takes precedent
  class(layer_conv_2d(NULL, 1, 1))[1]
pooling_layer_s3_classname <- 
  class(layer_max_pooling_2d(NULL))[1]

```

```{r}
is_conv_layer <- function(x) inherits(x, conv_layer_s3_classname)
is_pooling_layer <- function(x) inherits(x, pooling_layer_s3_classname)

# Extract outputs from Conv2d and MaxPooling layers and put into named list
layer_outputs <- list()
for (layer in model$layers)
  if (is_conv_layer(layer) || is_pooling_layer(layer))
    layer_outputs[[layer$name]] <- layer$output

# Create model that returns these outputs given the model inputs
activation_model <- keras_model(inputs = model$inputs,
                                outputs = layer_outputs)
```

Use the model to compute layer activations

```{r}
activations <- activation_model |>
  predict(img_tensor[tf$newaxis, , ,])
```

-   `predict()` returns a list of 9 R arrays, one per layer activation

-   Calling `[tf$newaxis, , , ]` changes `img_tensor`'s shape from (180, 180, 3) to (1, 180, 180, 3).

    -   It adds a batch dimension because the model expects the input to be a batch of images, not a single image.

Since we passed a name list for `outputs` when building the model, we get back a named list of R arrays when calling `predict()` on the model:

```{r}
str(activations)
```

Looking closer at the first layer activation:

```{r}
first_layer_activation <- activations[[ names(layer_outputs[1]) ]]
dim(first_layer_activation)
```

-   The first layer activation is a 178 x 178 feature map with 32 channels.

We can plot the firth channel of the activation of the first layer of the original model:

```{r}
plot_activations <- function(x, ...){
  # Convert tensors into arrays
  x <- as.array(x)
  
  # All-zero channels (no activations) are plotted as a gray rectangle
  if(sum(x) == 0)
    return(plot(as.raster("gray")))
  
  # Rotate the image clockwise to make it easier to view
  rotate <- function(x) t(apply(x, 2, rev))
  image(rotate(x), 
        asp = 1,
        axes = FALSE, 
        useRaster = TRUE,
        col = terrain.colors(256), ...) 
}

# Plot fith channel
plot_activations(first_layer_activation[, , , 5])
```

-   This channel appears to encode a diagonal edge detector.

    -   Note: Channels vary as the filters learned by convolution layers aren't deterministic

#### Visualize every change in every intermediate activation

We can plot the complete visualization of all the activations in the network.

-   We'll start by extracting every channel in each of the layer activation and then stacking the results into one big grid where the channels are stacked side by side

```{r}
# Iterate over the activations (and names of the corresponding layers)
for (layer_name in names(layer_outputs)) {
  layer_output <- activations[[layer_name]]
  
  # The layer activation has shape (1, height, width, n_features)
  n_features <- dim(layer_output) |> tail(1)
  # Prepare to display all of the channels of the activation in one plot
  par(mfrow = n2mfrow(n_features,
                      asp = 1.75),
                      mar = rep(.1, 4),
                      oma = c(0, 0, 1.5, 0))
  
  for (j in 1:n_features)
    plot_activations(layer_output[, , , j])
  title(main = layer_name, outer = TRUE)
}
```

We can see that the first layer's (`conv2d_9`) captures edges of the image and the activations retain almost the same information as the original picture.

-   The deeper the layer, the more abstract as the activations capture aspects related to the class rather then the visual content.

### Visualizing convnet filters

We'll inspect the filters learned by the convnets with gradient ascent.

Using the filters from the Xception model, we'll:

-   Build a loss function that maximizes the value of a given filter in a given layer

-   Use stochastic gradient descent to adjust the values of the input image as to maximize the activation value.

#### Instantiate the Xception conv_base

```{r}
model <- application_xception(
  weights = "imagenet",
  include_top = FALSE
)
```

-   Classification layers for this example are irrelevant, so we don't need to include the top

We're interested in the convolutional layers of the model (`Conv2d` and `SeparatableConv2D`).

-   In order to get their outputs we'll need to know their names.

#### Printing names of all convolutional layers by depth

```{r}
for (layer in model$layers)
  # If a layer has a class Cov2D
  if (any(grepl("Conv2D", class(layer))))
    print(layer$name)
```

-   The separable convolutional layers in Xception are structure into blocks.

We can now create a second model, a feature extractor, that returns the output of a specific layer. Since the model uses the Functional API, it's inspect-able, and therefore can query the output and reuse it (no need to copy).

#### Creating a feature extractor model

```{r}
# Extract a layer based on the layers name (e.g. block3_sepconv1)
layer_name <- "block3_sepconv1"
layer <- model |> get_layer(name = "block3_sepconv1")
feature_extractor <- keras_model(inputs = model$input,
                                 outputs = layer$output)

```

-   This will create a model that returns the output of the target layer based on the input image.

#### Using the feature extractor

```{r}
activation <- img_tensor |>
  _[tf$newaxis, , , ] |>
  application_preprocess_inputs(model = model) |>
  feature_extractor()

str(activation)
  
```

```{r}
str(img_tensor)
layer$name

layer_name
```
