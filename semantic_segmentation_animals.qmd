---
title: "Semantic image segmentation of cats and dogs"
format: html
editor: visual
---

# Semantic segmentation using images of cats and ogs

In this project, we'll be using semantic segmentation to isolate cats and dogs from their background images.

-   The dataset will be the [Oxford-IIIT Pets database](http://www.robots.ox.ac.uk/~vgg/%20data/pets/), which contains over 7,390 imafgbtg of different cats and dogs.

    -   Each of these images are paired with foreground-background segmentation masks

    -   The pixel of the segmentation masks will have one of three values:

        -   Foreground: 1

        -   Background: 2

        -   Contour: 3

## Packages

```{r}
library(fs)
library(keras3)
library(tensorflow)
library(tfdatasets)
```

Wake up `reticulate` and `set.seed()`:

```{r}
op_add(1, 1)
set.seed(123)
```

## Data

First, we'll need to download and uncompressed the dataset using the `download.file()` and `untar()` utilities in R. We'll also use the `fs` package for filesystem operations.

```{r}
# Create directory
data_dir <- path("pets_dataset")
dir_create(data_dir)
```

```{r, eval = FALSE}
options(timeout=100)
data_url <- path("https://thor.robots.ox.ac.uk/~vgg/data/pets")
for (filename in c("images.tar.gz", "annotations.tar.gz")) {
  download.file(url =  data_url / filename,
                destfile = data_dir / filename)
  untar(data_dir / filename, exdir = data_dir)
}

```

-   The input pictures are stored as JPG files and the corresponding segmentation masks are stored as PNG files.

Next, we'll create a tibble with columns for the input file paths and the corresponding mask file paths:

```{r}
input_dir <- data_dir / "images"
target_dir <- data_dir / "annotations/trimaps/"

image_paths <- tibble::tibble(
  # Sort list so that the input matches the target
  input = sort(dir_ls(input_dir, glob = "*.jpg")),
  target = sort(dir_ls(target_dir, glob = "*.png"))
)
```

-   `dir_ls()` is equivalent to the `ls` command and returns file names as a named `fs_path` character vector.
-   This will keep track of the paths and make sure both the input and target vectors stay in sync.

```{r, eval = FALSE}
# Check that image_paths looks right
tibble::glimpse(image_paths)
```

### View image and its mask

We can view an image and its corresponding mask using the TensorFlow API. The first step will be to create a helper function that will plot a TensorFlow Tensor containing an image using R's `plot()` function:

```{r, eval = FALSE}
display_image_tensor <- function(x, ..., max = 255, plot_margins = c(0, 0, 0, 0)){
  # If there are plot_margins
  if(!is.null(plot_margins)) {
    # Default is no margins when plotting images
    par(mar = plot_margins)
  }
  
  x %>%
    # Convert TF Tensor to R array
    as.array() %>%
    # Remove axes that are size 1
    drop() %>%
    # Convert R array into raster object
    as.raster(max = max) %>%
    plot(..., interpolate = FALSE)
}
```

-   `drop()` example: If x is a grayscale image with one color channel, it would squeeze the tensor shape from (height, width, 1) to (height, width)

-   We use `as.raster(max = max)`, which is 255 because the images are encoded as `uint8`, which only have a range from \[0, 255\]

    -   By setting `max = 255`, R will plot the 255 pixels as white and 0 as black and then interpolate linearly for values between them as shades of grey.

-   `interpolate = FALSE` tells R to draw pixels with sharp edges, no blending or interpolation of colors between pixels.

We can now read an image into a Tensor and view it:

```{r, eval = FALSE}
# View 10th image
image_tensor <- image_paths$input[10] %>%
  tf$io$read_file() %>%
  tf$io$decode_jpeg()

# Verify that the image is loaded into a Tensor
str(image_tensor)

# View image in tensor
display_image_tensor(image_tensor)
```

To see the target image, the segmentation mask, we can create another helper function:

-   Note: The target images are still `uint8` but they only have values of `(1, 2, 3)`

    -   To plot it, because this is a Tensor being converted to an R array, we need to subtract 1 in order to get it into Python's sill `(0, 1, 2)`

        -   0: Black

        -   1: Gray

        -   2: White

```{r}
display_target_tensor <- function(target){
  display_image_tensor(target - 1, max = 2)
}
```

To view the mask of the 10th image:

```{r, eval = FALSE}
target <- image_paths$target[10] |>
  tf$io$read_file() |>
  tf$io$decode_png()

# View that it is a tensor
str(target)

# View image
display_target_tensor(target)
```

### Load files into TF Datasets and split into train/validation

We'll load the input and targets into two TF Datasets and then split them into training and validation sets.

-   Since the dataset is pretty small, this can all be loaded into memory

```{r}
# Helper to read and resize images using TensorFlow operations
tf_read_image <-
  function(path, format = "image", resize = NULL, ...){
    img <- path |>
      tf$io$read_file() |>
      # Look up decode_image(), decode_jpg, or decode_png() from tf$io submodule
      tf$io[[paste0("decode_", format)]](...)
    
    if(!is.null(resize))
      img <- img |>
        # Rememver: tf module functions use integers so we need as.integer()
        tf$image$resize(as.integer(resize))
    
    return(img)
  }

# Image size will be 200 x 200
img_size <- c(200, 200)

tf_read_image_and_resize <- function(..., resize = img_size){
  tf_read_image(..., resize = resize)
}
```

```{r}
# Function to create dataset from images and store in RAM 
make_dataset <- function(paths_df){
  tensor_slices_dataset(paths_df) |>
    dataset_map(function(path){
      image <- path$input |>
        # Each input image has three channels (RGB)
        tf_read_image_and_resize("jpeg", channels = 3L)
      target <- path$target |>
        # Each target image has a single channel for each pixel
        tf_read_image_and_resize("png", channels = 1L)
      # Subtract 1 to get labels to (0, 1, 2)...silly Python..
      target <- target - 1
      list(image, target)
    }) |>
    dataset_cache() |>
    dataset_shuffle(buffer_size = nrow(paths_df)) |>
    dataset_batch(32)
}
```

-   The R function passed to `dataset_map()` is called with a symbolic tensor, which means it has to return one as well.

    -   `dataset_map()` takes in a single argument, a named list of two scalar string tensors. Each of these contain paths to the input and target images.

-   Caching the dataset store the full dataset in RAM after the first run.

    -   If the dataset is too big, just remove the `dataset_cache()` and the image files will loaded as needed during training.

Now we need to split the dataset into training and validation sets. We'll allocate 1000 samples for validation and the rest will be for training.

```{r}
# Reserver 1000 samples for validation
num_val_samples <- 1000
val_idx <- sample.int(nrow(image_paths), num_val_samples)

# Split into training and validation sets
val_paths <- image_paths[val_idx, ]
# Everyting not in the validation set goes into the training set
train_paths <- image_paths[-val_idx, ]

# Create datasets
validation_dataset <- make_dataset(val_paths)
train_dataset <- make_dataset(train_paths)
```

### Define the model

```{r}
get_model <- function(img_size, num_classes){
  # Local functions to avoid passing the same arguments to each call
  conv <- function(..., padding = "same", activation = "relu") {
    layer_conv_2d(..., padding = padding, activation = activation)
  }
  conv_transpose <- function(..., padding = "same", activation = "relu") {
    layer_conv_2d_transpose(..., padding = padding, activation = activation)
  }
  
  input <- layer_input(shape = c(img_size, 3))
  output <- input |>
    # Rescale input image to [0, 1] range
    layer_rescaling(scale = 1/255) |>
    conv(64, 3, strides = 2) %>%
    conv(64, 3) %>%
    conv(128, 3, strides = 2) %>%
    conv(128, 3) %>%
    conv(256, 3, strides = 2) %>%
    conv(256, 3) %>%
    conv_transpose(256, 3) %>%
    conv_transpose(256, 3, strides = 2) %>%
    conv_transpose(128, 3) %>%
    conv_transpose(128, 3, strides = 2) %>%
    conv_transpose(64, 3) %>%
    conv_transpose(64, 3, strides = 2) %>%
    conv(num_classes, 3, activation = "softmax")

  keras_model(input, output)
  
}
```

-   Use `padding = "same"` to avoid influence of the border padding on the feature map size

-   The model ends with a per-pixel 3-way softmax to classify each output pixel as one of the three categories

```{r}
model <- get_model(img_size = img_size, num_classes = 3)

model
```

-   Using strides instead of `MaxPooling2D` layers for downsampling in order to preserve spatial location information in the image

### Compile and fit the model

```{r}
# Compile the model
model %>%
    compile(optimizer = "rmsprop",
            loss = "sparse_categorical_crossentropy")

# Call back that saves the best model
callbacks <- list(
  callback_model_checkpoint("oxford_segmentation.keras",
                            save_best_only = TRUE))
# Fit the model
history <- model %>% fit(
  train_dataset,
  epochs = 25,
  callbacks = callbacks,
  validation_data = validation_dataset
)
```

-   Note: It's possible to get a warning during training to the effect of "`Corrupt JPEG data: premature end of the data segment"`. The image dataset isn't perfect, but the `tf$io` functions can usually recover gracefully.

We can display the training and validation loss of the model:

```{r}
plot(history)
```

-   I happen to know that it overfitting starts around 25 epochs, so I only went 25 to save time.

We reload the best performing model (based on validation loss) to see how it would predict a segmentation mask:

```{r}
# Load best performing model
model <- load_model("oxford_segmentation.keras")

# Select image 102 to predict its mask
test_image <- val_paths$input[102] %>%
  tf_read_image_and_resize("jpeg", channels = 3L)

predicted_mask_probs <-
  model(test_image[tf$newaxis, , , ])

# predicted_mask is a tesnro with shape=(1, 200, 200), dtype = int64
predicted_mask <-
  tf$argmax(predicted_mask_probs, axis = -1L)

predicted_target <- predicted_mask + 1

par(mfrow = c(1, 2))
display_image_tensor(test_image)
display_target_tensor(predicted_target)
```

-   `tf$argmax()` is similar to `which.max()` in R.

    -   A key difference is that `tf$argmax()` returns 0-based values

### Mini Xception-like model

For this example, we'll create a smaller version of Xception and apply it to the dogs vs cats classification task that we did previously. In this example, we'll replace the model definition with the following convnet:

```{r}
data_augmentation <- keras_model_sequential() %>%
  layer_random_flip("horizontal") %>%
  layer_random_rotation(0.1) %>%
  layer_random_zoom(0.2)

inputs <- layer_input(shape = c(180, 180, 3))

x <- inputs %>%
  data_augmentation() %>%
  layer_rescaling(scale = 1 / 255)

x <- x %>%
  layer_conv_2d(32, 5, use_bias = FALSE)

for (size in c(32, 64, 128, 256, 512)) {
  residual <- x
  
  x <- x %>%
    layer_batch_normalization() %>%
    layer_activation("relu") %>%
    
    layer_separable_conv_2d(size, 3, padding = "same", use_bias = FALSE) %>%
    layer_batch_normalization() %>%
    layer_activation("relu") %>%

    layer_separable_conv_2d(size, 3, padding = "same", use_bias = FALSE) %>%
    layer_max_pooling_2d(pool_size = 3, strides = 2, padding = "same")

  residual <- residual %>%
    layer_conv_2d(size, 1, strides = 2, padding = "same", use_bias = FALSE)
  x <- layer_add(list(x, residual))
}

outputs <- x %>%
  layer_global_average_pooling_2d() %>%
  layer_dropout(0.5) %>%
  layer_dense(1, activation = "sigmoid")

model <- keras_model(inputs, outputs)

train_dataset <- image_dataset_from_directory(
  "cats_vs_dogs_small/train",
  image_size = c(180, 180),
  batch_size = 32
)

validation_dataset <- image_dataset_from_directory(
  "cats_vs_dogs_small/validation",
  image_size = c(180, 180),
  batch_size = 32
)

model %>%
  compile(
  loss = "binary_crossentropy",
  optimizer = "rmsprop",
  metrics = "accuracy"
)

history <- model %>%
  fit(train_dataset,
      epochs = 100,
      validation_data = validation_dataset)
```

```{r}
plot(history)
```

The mini-Xception model is able to achieve a test accuracy of 90.8%, whereas the naive model achieved an accuracy of 81.4%
