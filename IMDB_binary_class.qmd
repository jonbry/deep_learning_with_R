---
title: "IMDB_binary_classification"
format: html
editor: visual
---

## IMDB Binary Classification

The IMDB dataset from Keras comes with 50,000 polarized views (positive or negative). The test/train split is 50/50 and the data has already been preprocessed (reviews are a list of integers). The ratio of positive and negative reviews are split evenly in each of the datasets,

#### Packages

```{r}
library(keras)
```

#### Load the IMDB dataset

```{r}
imdb <- dataset_imdb(num_words = 10000)
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb
```

-   Note: If you get an error about the SSL certificate failing, you can likely fix the issue by running the `install Certficate.command` file in the python application folder. More info [here](https://bugs.python.org/issue28150)

-   The label lists positive reviews are 1 and negative reviews are 0

When loading the data, we used `%<-%` which is the multi-assignment operator from the `zeallot` package.

-   This unpacks a list into a set of distinct variables but could also be written as

```{r, eval = FALSE}
imdb <- dataset_imdb(num_words = 10000)
train_data <- imdb$train$x
train_labels <- imdb$train$y
test_data <- imdb$test$x
test_labels <- imdb$test$y
```

Inspecting the data and labels:

```{r}
str(train_data[[1]])
train_labels[[1]]
```

If we wanted to transform the reviews back into English, we can use the following for the first review:

```{r}
# named list that maps words to an integer index
word_index <- dataset_imdb_word_index()
# Maps integer index to words
reverse_word_index <- names(word_index)

# Decodes the review
names(reverse_word_index) <- word_index
decoded_review <- sapply(train_data[[1]], function(index){
  word <- if (index >= 3) reverse_word_index[[as.character(index - 3)]]
  if (!is.null(word)) word
  else "?"
})
```

-   The review is offset by 3 because 0, 1, and 2 are reserved indices for "padding", "start of sequence" and "unknown"

## Preparing the data

We need to transform the lists into tensors, which can be done by one-hot encoding the list. Here, we'll encode the integer sequences into a binary matrix

```{r}
# Function to create matrix of shape (length(sequence), dimensions) with all zeros
vectorize_sequences <- function(sequences, dimension = 10000){
  results <- matrix(0, nrow = length(sequences), ncol = dimension)
  # Set specific indicies of results[i] to 1
  for (i in 1:length(sequences))
    results[i, sequences[[i]]] <- 1
  results
}

x_train <- vectorize_sequences(train_data)
x_test <- vectorize_sequences(test_data)

# Convert labels from numeric to integers
y_train <- as.numeric(train_labels)
y_test <- as.numeric(test_labels)
```

#### Building the network

Since the input data is vectors and the labels are scalars (1s and zeros) we can use a simple stack of fully connected (dense) layers with `relu` activations: `layer_dense(units = 16, activation = "relu"`

```{r}
model <- keras_model_sequential() %>%
	layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>%
	layer_dense(units = 16, activation = "relu") %>%
	layer_dense(units = 1, activation = "sigmoid")
```

#### Configuring the optimizer

```{r}
model %>% compile(
	optimizer = "rmsprop",
	loss = "binary_crossentropy",
	metrics = c("accuracy")
)
```

#### Validation Set

```{r}
val_indices <- 1:10000

x_val <- x_train[val_indices,]
partial_x_train <- x_train[-val_indices,]

y_val <- y_train[val_indices]
partial_y_train <- y_train[-val_indices]
```

#### Fit the model

```{r}
history <- model %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)
```

```{r}
plot(history)
```

#### Retrain model with 4 epochs

```{r}
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

model %>% fit(x_train, y_train, epochs = 4, batch_size = 512)
results <- model %>% evaluate(x_test, y_test)

```

-   The model had an accuracy of \~89% with two hidden layers and 16 units.

#### Using trained network to generate predictions on new data

We can generate the likelihood of reviews being positive by using `predict` :

```{r}
model %>% predict(x_test[1:10,])
```

#### Adjust number of hidden layers

What happens if we just use 1 hidden layer?

```{r}
model2 <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>%
  layer_dense(units = 1, activation = "sigmoid")

model2 %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history2 <- model2 %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)
```

-   Slightly higher accuracy 0.893 vs 0.888, which is not significant.

What if we use 3 hidden layers?

```{r}
model_3layer <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model_3layer %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history_3layer <- model_3layer %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val))
```

-   Adding one more hidden layer (3) decreases the accuracy slightly, but not by a significant amount (0.881 vs 0.888)

#### Adjust number of hidden units

##### 32 hidden units:

```{r}
model_32unit <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = "relu", input_shape = c(10000)) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model_32unit %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history_32unit <- model_32unit %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val))
```

-   Increasing the hidden units to 32 decreases the accuracy from 0.888 to 0.885, which is not significant.

##### 64 hidden units:

```{r}
model_64unit <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu", input_shape = c(10000)) %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model_64unit %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history_64unit <- model_64unit %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val))
```

-   Roughly the same decrease as 32 hidden units (0.888 vs 0.885)

#### Using MSE loss function instead of binary cross-entropy

```{r}
model_mse <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model_mse %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("accuracy")
)

history_mse <- model_mse %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val))
```

-   Changing the loss function to mse had a very very small effect on the accuracy and can be considered the same.

#### Using tanh activation instead of relu

```{r}
model_tanh <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "tanh", input_shape = c(10000)) %>%
  layer_dense(units = 16, activation = "tanh") %>%
  layer_dense(units = 1, activation = "sigmoid")

model_tanh %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history_tahn <- model_tanh %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val))
```

-   Changing the activation from `relu` to `tanh` had no real impact on the accuracy on the validation data set. The results were essentially the same.
