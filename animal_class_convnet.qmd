---
title: "Dog/Cat classification using Convnet"
format: html
editor: visual
---

## Classifying Dogs and Cats using Convnet

For this project, we'll use a small dataset of 5000 images of cats and dogs (50/50 split). - We'll use 2000 images for training, 1000 for validation, and 2000 for testing.

### Plan of action

1.  Naively train a small convnet on the training sample without any regularization to set a baseline (\~70%)
2.  Use data augmentation to mitigate overfitting for computer vision (improve \~80-85%)
3.  Apply feature extraction with pre-trained model (\~97.5%)
4.  Apply fine-tuning a pre-trained model (\~98.5%)

```{r}
library(fs)
library(keras3)
```

### Data

We'll download the data from Kaggle through their API.

```{r, eval = FALSE}
library(fs) dir_create("~/.kaggle") 
file_move("~/Downloads/kaggle.json", "~/.kaggle/") 
file_chmod("~/.kaggle/kaggle.json", "0600")

reticulate::py_install("kaggle", pip = TRUE)
```

```{r, eval = FALSE}
# Force reticulate to initialize python
op_add(1, 1)

# Now 'kaggle' should be on the PATH
nzchar(Sys.which("kaggle"))
system('kaggle competitions download -c dogs-vs-cats')
```

-   Note: I marked dog-vs-cats in gitignore before opening RStudio because it will slow things down

#### Create training, validation, and test sets

There are 25K images of dogs and cats (split 50/50), which is more than we actually need. To make this easier, we'll split the images into the following groups (evenly split)

-   Train - 2000 images

-   Validation - 1000 images

-   Test - 2000 images

```{r}
# Path to the original data set
original_dir <- path("dogs-vs-cats/train")
# Directory for smaller dataset
new_base_dir <- path("cats_vs_dogs_small")

# Function that copies images between start_index and end_index to the new subdirectory. The subset_name is either train, validation, or test
make_subset <- function(subset_name, start_index, end_index){
  for (category in c("dog", "cat")) {
    file_name <- glue::glue("{category}.{ start_index:end_index }.jpg")
    dir_create(new_base_dir / subset_name / category)
    file_copy(original_dir / file_name,
              new_base_dir / subset_name / category /file_name)
  }
}

# Create training subset with first 1000 images of each category
make_subset("train", start_index = 1, end_index = 1000)
# Create validation subset with next 500 images of each category
make_subset("validation", start_index = 1001, end_index = 1500)
# Create test subset with next 1000 images of each category
make_subset("test", start_index = 1501, end_index = 2500)


```

-   Also added to gitignore

### Building the model

Since this is a binary-classification problem, we'll end with a single unit (`layer_dense()` of size 1) and a `sigmoid` activation

We will start the model with `layer_rescaling()`, which will rescale the image input ranges from \[0, 255\] to \[0, 1\]

```{r}
inputs <- layer_input(shape = c(180, 180, 3))
  outputs <- inputs %>%
  # Rescale inputs to the [0, 1] range by dividing them by 255.
  layer_rescaling(1 / 255) %>%
  layer_conv_2d(filters = 32, kernel_size = 3, activation = "relu") %>%
  layer_max_pooling_2d(pool_size = 2) %>%
  layer_conv_2d(filters = 64, kernel_size = 3, activation = "relu") %>%
  layer_max_pooling_2d(pool_size = 2) %>%
  layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu") %>%
  layer_max_pooling_2d(pool_size = 2) %>%
  layer_conv_2d(filters = 256, kernel_size = 3, activation = "relu") %>%
  layer_max_pooling_2d(pool_size = 2) %>%
  layer_conv_2d(filters = 256, kernel_size = 3, activation = "relu") %>%
  layer_flatten() %>%
  layer_dense(1, activation = "sigmoid")
model <- keras_model(inputs, outputs)

model
```

For the compilation step, we'll use the `RMSprop` and `binary_crossentropy` for the loss since we're ending with a single sigmoid unit.

```{r}
 model %>% compile(loss = "binary_crossentropy",
                          optimizer = "rmsprop",
                          metrics = "accuracy")
```

### Data pre-processing

The JPEG images will need to be pre-processed into floating-point tensors, which is done through the following steps:

1.  Read in the picture files
2.  Convert the JPEG content to RGB grids of pixels
3.  Convert grids into floating-point tensors
4.  Resize the tensors to a shared size (e.g. 180 x 180)
5.  Pack the tensors into batches (e.g. 32)

Even though these are only 5 steps, it ends up being quite a bit of code. To speed things up, Keras has the utility function `image_dataset_from_directory()`, which automates the pipeline from images to pre-processed tensors.

Calling `image_dataset_from_directory()` will:

-   First list the subdirectories of `directory` and assume each one contains images from one of the classes

-   Next, it will then index the image files for each directory

-   Finally it will create and return a TF Dataset object to read the files, shuffle them, decode them into tensors, resize them, and pack them into batches

```{r}
train_dataset <-
  image_dataset_from_directory(new_base_dir / "train",
                               image_size = as.integer(c(180, 180)),
                               batch_size = 32)
validation_dataset <-
  image_dataset_from_directory(new_base_dir / "validation",
                               image_size = as.integer(c(180, 180)),
                               batch_size = 32)
test_dataset <-
  image_dataset_from_directory(new_base_dir / "test",
                               image_size = as.integer(c(180, 180)),
                               batch_size = 32)
```

The Dataset objects creates batches of 180 x 180 RGB images (shape `(32, 180, 180, 3)` and integer labels (shape `(32)`).

We can display the shapes of the data and labels created by the Dataset object:

```{r}
c(data_batch, labels_batch) %<-% iter_next(as_iterator(train_dataset))
data_batch$shape

labels_batch$shape
```

### Fit the model

We will use the `validation_data` argument in `fit()` to monitor the validation metrics in a separate TF Dataset object.

We'll also use `callback_model_checkpoint()` to save the model after each epoch.

-   Using the argument `save_best_only = TRUE` will keep track of the best performing model and save it for later use (no need to eyeball it)

-   The argument `monitor = "val_loss"` will monitor the val_loss so that it can save the model with the lowest validation loss.

Together, we will be saving the state of the model that has the best-performing epoch on the validation data.

-   Afterwards, we won't need to retrain a new model, we can just reload the saved one.

```{r}
# Callback setup
callbacks <- list(
  callback_model_checkpoint(
    filepath = "convnet_from_scratch.keras",
    save_best_only = TRUE,
    monitor = "val_loss"
  )
)

history <- model %>%
  fit(train_dataset,
      epochs = 30,
      validation_data = validation_dataset,
      callbacks = callbacks)
```

We can plot the results:

```{r}
plot(history)
```
