---
title: "Dog/Cat classification using Convnet"
format: html
editor: visual
---

## Classifying Dogs and Cats using Convnet

For this project, we'll use a small dataset of 5000 images of cats and dogs (50/50 split). - We'll use 2000 images for training, 1000 for validation, and 2000 for testing.

### Plan of action

1.  Naively train a small convnet on the training sample without any regularization to set a baseline (\~70%)
2.  Use data augmentation to mitigate overfitting for computer vision (improve \~80-85%)
3.  Apply feature extraction with pre-trained model (\~97.5%)
4.  Apply fine-tuning a pre-trained model (\~98.5%)

```{r}
library(fs)
library(keras3)
library(listarrays)
library(reticulate)
library(tfdatasets)
```

### Data

We'll download the data from Kaggle through their API.

```{r, eval = FALSE}
dir_create("~/.kaggle") 
file_move("~/Downloads/kaggle.json", "~/.kaggle/") 
file_chmod("~/.kaggle/kaggle.json", "0600")

reticulate::py_install("kaggle", pip = TRUE)
```

```{r, eval = FALSE}
# Force reticulate to initialize python
op_add(1, 1)

# Now 'kaggle' should be on the PATH
nzchar(Sys.which("kaggle"))
system('kaggle competitions download -c dogs-vs-cats')
```

-   Note: I marked dog-vs-cats in gitignore before opening RStudio because it will slow things down

#### Create training, validation, and test sets

There are 25K images of dogs and cats (split 50/50), which is more than we actually need. To make this easier, we'll split the images into the following groups (evenly split)

-   Train - 2000 images

-   Validation - 1000 images

-   Test - 2000 images

```{r}
# Path to the original data set
original_dir <- path("dogs-vs-cats/train")
# Directory for smaller dataset
new_base_dir <- path("cats_vs_dogs_small")

```

```{r, eval=FALSE}
# Only needs to be run once
# Function that copies images between start_index and end_index to the new subdirectory. The subset_name is either train, validation, or test
make_subset <- function(subset_name, start_index, end_index){
  for (category in c("dog", "cat")) {
    file_name <- glue::glue("{category}.{ start_index:end_index }.jpg")
    dir_create(new_base_dir / subset_name / category)
    file_copy(original_dir / file_name,
              new_base_dir / subset_name / category /file_name)
  }
}

# Create training subset with first 1000 images of each category
make_subset("train", start_index = 1, end_index = 1000)
# Create validation subset with next 500 images of each category
make_subset("validation", start_index = 1001, end_index = 1500)
# Create test subset with next 1000 images of each category
make_subset("test", start_index = 1501, end_index = 2500)

```

-   Also added to gitignore

### Building the model

Since this is a binary-classification problem, we'll end with a single unit (`layer_dense()` of size 1) and a `sigmoid` activation

We will start the model with `layer_rescaling()`, which will rescale the image input ranges from \[0, 255\] to \[0, 1\]

```{r}
inputs <- layer_input(shape = c(180, 180, 3))
  outputs <- inputs %>%
  # Rescale inputs to the [0, 1] range by dividing them by 255.
  layer_rescaling(1 / 255) %>%
  layer_conv_2d(filters = 32, kernel_size = 3, activation = "relu") %>%
  layer_max_pooling_2d(pool_size = 2) %>%
  layer_conv_2d(filters = 64, kernel_size = 3, activation = "relu") %>%
  layer_max_pooling_2d(pool_size = 2) %>%
  layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu") %>%
  layer_max_pooling_2d(pool_size = 2) %>%
  layer_conv_2d(filters = 256, kernel_size = 3, activation = "relu") %>%
  layer_max_pooling_2d(pool_size = 2) %>%
  layer_conv_2d(filters = 256, kernel_size = 3, activation = "relu") %>%
  layer_flatten() %>%
  layer_dense(1, activation = "sigmoid")
model <- keras_model(inputs, outputs)

model
```

For the compilation step, we'll use the `RMSprop` and `binary_crossentropy` for the loss since we're ending with a single sigmoid unit.

```{r}
 model %>% compile(loss = "binary_crossentropy",
                          optimizer = "rmsprop",
                          metrics = "accuracy")
```

### Data pre-processing

The JPEG images will need to be pre-processed into floating-point tensors, which is done through the following steps:

1.  Read in the picture files
2.  Convert the JPEG content to RGB grids of pixels
3.  Convert grids into floating-point tensors
4.  Resize the tensors to a shared size (e.g. 180 x 180)
5.  Pack the tensors into batches (e.g. 32)

Even though these are only 5 steps, it ends up being quite a bit of code. To speed things up, Keras has the utility function `image_dataset_from_directory()`, which automates the pipeline from images to pre-processed tensors.

Calling `image_dataset_from_directory()` will:

-   First list the subdirectories of `directory` and assume each one contains images from one of the classes

-   Next, it will then index the image files for each directory

-   Finally it will create and return a TF Dataset object to read the files, shuffle them, decode them into tensors, resize them, and pack them into batches

```{r}
train_dataset <-
  image_dataset_from_directory(new_base_dir / "train",
                               image_size = as.integer(c(180, 180)),
                               batch_size = 32)
validation_dataset <-
  image_dataset_from_directory(new_base_dir / "validation",
                               image_size = as.integer(c(180, 180)),
                               batch_size = 32)
test_dataset <-
  image_dataset_from_directory(new_base_dir / "test",
                               image_size = as.integer(c(180, 180)),
                               batch_size = 32)
```

The Dataset objects creates batches of 180 x 180 RGB images (shape `(32, 180, 180, 3)` and integer labels (shape `(32)`).

We can display the shapes of the data and labels created by the Dataset object:

```{r}
c(data_batch, labels_batch) %<-% reticulate::iter_next(reticulate::as_iterator(train_dataset))
data_batch$shape

labels_batch$shape
```

### Fit the model

We will use the `validation_data` argument in `fit()` to monitor the validation metrics in a separate TF Dataset object.

We'll also use `callback_model_checkpoint()` to save the model after each epoch.

-   Using the argument `save_best_only = TRUE` will keep track of the best performing model and save it for later use (no need to eyeball it)

-   The argument `monitor = "val_loss"` will monitor the val_loss so that it can save the model with the lowest validation loss.

Together, we will be saving the state of the model that has the best-performing epoch on the validation data.

-   Afterwards, we won't need to retrain a new model, we can just reload the saved one.

```{r}
# Callback setup
callbacks <- list(
  callback_model_checkpoint(
    filepath = "convnet_from_scratch.keras",
    save_best_only = TRUE,
    monitor = "val_loss"
  )
)

history <- model %>%
  fit(train_dataset,
      epochs = 30,
      validation_data = validation_dataset,
      callbacks = callbacks)
```

We can plot the results:

```{r}
plot(history)
```

We can see overfitting, which is what we want.

-   The accuracy for training hits almost 100% and validation tops out at around 75% after \~10 epochs.

To check the test accuracy we can reload the model from the saved file to evaluate it:

```{r}
test_model <- load_model_tf("convnet_from_scratch.keras")
  result <- evaluate(test_model, test_dataset)
  cat(sprintf("Test accuracy: %.3f\n", result["accuracy"]))
```

-   The accuracy on the test dataset was 71.4% (answers will change due to randomization the initializations of the neural network.

### Adding data augmentation

To improve generalization, we'll use data augmentation to increase the training sample size, but the model will never see the exact same picture twice. This is done by add random transformations to existing training samples.

-   We'll add [data augmentation layers]{.underline} to the beginning of the model, following a `keras_model_sequential()` layer and before the `layer_rescaling()`:

```{r}
data_augmentation <- keras_model_sequential() |>
  layer_random_flip("horizontal") |>
  layer_random_rotation(0.1) |>
  layer_random_zoom(0.2)
```

-   `layer_random_flip("horizontal")` applies a horizontal flip to a random 50% of the images being processed

-   `layer_random_rotation(0.1)` rotates the images by a random value in the range of `[-10%, +10%]`

    -   These are fractions of a full circle. In degrees it would be `[-36 deg, +36 deg]`

-   `layer_random_zoom(0.2)` zooms in or out of an image by a random factor in the range of `[-20%, +20%]`

Below is an example of the augmented images:

```{r}
library(tfdatasets)
batch <- train_dataset %>%
  as_iterator() %>%
  iter_next()

c(images, labels) %<-% batch
# Prepare the graphics device for nine images
par(mfrow = c(3, 3), mar = rep(0.5, 4))

# Plot the first image of the batch without augmentation
image <- images[1, , , ]
plot(as.raster(as.array(image), max = 255))

for (i in 2:9) {
  # Apply augmentation stage to each batch of images
  augmented_images <- data_augmentation(images)
  augmented_image <- augmented_images[1, , , ]
  # Display the first image in the output batch
  plot(as.raster(as.array(augmented_image), max = 255))
}
```

To further decrease overfitting, we can add a `layer_dropout()` before the densely connected layer:

```{r}
inputs <- layer_input(shape = c(180, 180, 3))
outputs <- inputs %>%
  data_augmentation() %>%
  layer_rescaling(1 / 255) %>%
  layer_conv_2d(filters = 32, kernel_size = 3, activation = "relu") %>%
  layer_max_pooling_2d(pool_size = 2) %>%
  layer_conv_2d(filters = 64, kernel_size = 3, activation = "relu") %>%
  layer_max_pooling_2d(pool_size = 2) %>%
  layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu") %>%
  layer_max_pooling_2d(pool_size = 2) %>%
  layer_conv_2d(filters = 256, kernel_size = 3, activation = "relu") %>%
  layer_max_pooling_2d(pool_size = 2) %>%
  layer_conv_2d(filters = 256, kernel_size = 3, activation = "relu") %>%
  layer_flatten() %>%
  layer_dropout(0.5) %>%
  layer_dense(1, activation = "sigmoid")
model <- keras_model(inputs, outputs)
model %>% compile(loss = "binary_crossentropy",
                  optimizer = "rmsprop",
                  metrics = "accuracy")
```

If we train the model using data augmentation and dropout, we can expect overfitting to take much longer. For this reason, it makes sense to increase the number of epochs to 100:

```{r}
callbacks <- list(
  callback_model_checkpoint(
    filepath = "convnet_from_scratch_with_augmentation.keras",
    save_best_only = TRUE,
    monitor = "val_loss"
))

history <- model %>% fit(
    train_dataset,
    epochs = 100,
    validation_data = validation_dataset,
    callbacks = callbacks
)

```

```{r}
plot(history)
```

```{r}
test_model <- load_model("convnet_from_scratch_with_augmentation.keras") 
result <- evaluate(test_model, test_dataset)
cat(sprintf("Test accuracy: %.3f\n", result["accuracy"]))
```

### Leverage a pre-trained model

We'll use the convolutional base of the VGG16 model (published in 2014) to extract features from dog and cat images. We'll use the VGG16's convolutional base to extract features, and then train a dog-vs-cat classifier on top of those features.

#### Instantiate the VGG16 model

```{r}
conv_base <- application_vgg16(
  weights <- "imagenet",
  include_top = FALSE,
  input_shape = c(180, 180, 3)
)

conv_base
```

There are two ways of moving forward with feature extraction:

-   [Fast feature extraction without data augmentation]{.underline} — Run the convolutional base over the dataset, record the output (arrays) to a file, use the data as input to a standalone densely connected classifier.

    -   This is a fast and cheap solution because the convolutional base is only running once for every input image.

    -   The downside is that we can use data augmentation

-   [Feature extraction with data augmentation]{.underline} — Extend the model (`conv_base`) by adding a `Dense` layer on top and run the whole thing from the beginning.

    -   Much more expensive process, but it allows us to use data augmentation since every input image goes through the convolutional base every time it is seen by the model.

#### Fast feature extraction without data augmentation

The `predict()` method of the `conv_base` model can be used to extract features as R arrays. We'll use \`predict()\` on all datasets:

```{r}
get_features_and_labels <- function(model, dataset) {
  n_batches <- length(dataset)
  all_features <- vector("list", n_batches)
  all_labels <- vector("list", n_batches)
  iterator <- as_iterator(dataset)
  # iterator starts with zero
  #for (i in 1:n_batches) {
   for (i in 0){
    cat(i)
    c(images, labels) %<-% iter_next(iterator)
    summary(images)
    preprocessed_images <- application_preprocess_inputs(model, images)
    summary(preprocessed_images)
    features <- conv_base %>% predict(preprocessed_images)
    all_labels[[i+1]] <- labels
    all_features[[i+1]] <- features
   }
  
    # # all_features <- listarrays::bind_on_rows(all_features)
    # # all_labels <- listarrays::bind_on_rows(all_labels)
    # 
    # list(all_features, all_labels)
  return(all_features) # is a tensor
}

images_test <- get_features_and_labels(conv_base, train_dataset)
# c(train_features, train_labels) %<-% get_features_and_labels(conv_base, train_dataset)

```

```{r}

test_model <- application_vgg16(
  weights <- "imagenet",
  include_top = FALSE,
  input_shape = c(180, 180, 3)
)

app_vgg <- application_preprocess_inputs(test_model, train_dataset)
data_batch_test <- application_preprocess_inputs(test_model, data_batch)


```

```{r}


n_batches <- length(train_dataset)
all_features <- vector("list", n_batches)
all_labels <- vector("list", n_batches)
iterator <- as_iterator(train_dataset)
c(images, labels) %<-% iter_next(iterator)
pre_processed_image <- application_preprocess_inputs(test_model, images)


c(images, labels) %<-% iter_next(iterator)
preprocessed_images <- application_preprocess_inputs(test_model, images)
features <- conv_base %>% predict(preprocessed_images)
all_labels[[i]] <- labels
all_features[[i]] <- features


all_features <- listarrays::bind_on_rows(all_features)
all_labels <- listarrays::bind_on_rows(all_labels)

list(all_features, all_labels)

```

```{r}
model <- application_convnext_tiny()

inputer <- random_normal(c(32, 224, 224, 3))
processed_inputs <- application_preprocess_inputs(model, inputer)
```
