---
title: "Dog/Cat classification using Convnet"
format: html
editor: visual
---

## Classifying Dogs and Cats using Convnet

For this project, we'll use a small dataset of 5000 images of cats and dogs (50/50 split). - We'll use 2000 images for training, 1000 for validation, and 2000 for testing.

### Plan of action

1.  Naively train a small convnet on the training sample without any regularization to set a baseline (\~70%)
2.  Use data augmentation to mitigate overfitting for computer vision (improve \~80-85%)
3.  Apply feature extraction with pre-trained model (\~97.5%)
4.  Apply fine-tuning a pre-trained model (\~98.5%)

```{r}
library(fs)
library(keras3)
```

### Data

We'll download the data from Kaggle through their API.

```{r, eval = FALSE}
library(fs) dir_create("~/.kaggle") 
file_move("~/Downloads/kaggle.json", "~/.kaggle/") 
file_chmod("~/.kaggle/kaggle.json", "0600")

reticulate::py_install("kaggle", pip = TRUE)
```

```{r, eval = FALSE}
# Force reticulate to initialize python
op_add(1, 1)

# Now 'kaggle' should be on the PATH
nzchar(Sys.which("kaggle"))
system('kaggle competitions download -c dogs-vs-cats')
```

-   Note: I marked dog-vs-cats in gitignore before opening RStudio because it will slow things down

#### Create training, validation, and test sets

There are 25K images of dogs and cats (split 50/50), which is more than we actually need. To make this easier, we'll split the images into the following groups (evenly split)

-   Train - 2000 images

-   Validation - 1000 images

-   Test - 2000 images

```{r}
# Path to the original data set
original_dir <- path("dogs-vs-cats/train")
# Directory for smaller dataset
new_base_dir <- path("cats_vs_dogs_small")

# Function that copies images between start_index and end_index to the new subdirectory. The subset_name is either train, validation, or test
make_subset <- function(subset_name, start_index, end_index){
  for (category in c("dog", "cat")) {
    file_name <- glue::glue("{category}.{ start_index:end_index }.jpg")
    dir_create(new_base_dir / subset_name / category)
    file_copy(original_dir / file_name,
              new_base_dir / subset_name / category /file_name)
  }
}

# Create training subset with first 1000 images of each category
make_subset("train", start_index = 1, end_index = 1000)
# Create validation subset with next 500 images of each category
make_subset("validation", start_index = 1001, end_index = 1500)
# Create test subset with next 1000 images of each category
make_subset("test", start_index = 1501, end_index = 2500)


```

-   Also added to gitignore

### Building the model

Since this is a binary-classification problem, we'll end with a single unit (`layer_dense()` of size 1) and a `sigmoid` activation

We will start the model with `layer_rescaling()`, which will rescale the image input ranges from \[0, 255\] to \[0, 1\]

```{r}
inputs <- layer_input(shape = c(180, 180, 3))
  outputs <- inputs %>%
  # Rescale inputs to the [0, 1] range by dividing them by 255.
  layer_rescaling(1 / 255) %>%
  layer_conv_2d(filters = 32, kernel_size = 3, activation = "relu") %>%
  layer_max_pooling_2d(pool_size = 2) %>%
  layer_conv_2d(filters = 64, kernel_size = 3, activation = "relu") %>%
  layer_max_pooling_2d(pool_size = 2) %>%
  layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu") %>%
  layer_max_pooling_2d(pool_size = 2) %>%
  layer_conv_2d(filters = 256, kernel_size = 3, activation = "relu") %>%
  layer_max_pooling_2d(pool_size = 2) %>%
  layer_conv_2d(filters = 256, kernel_size = 3, activation = "relu") %>%
  layer_flatten() %>%
  layer_dense(1, activation = "sigmoid")
model <- keras_model(inputs, outputs)

model
```

For the compilation step, we'll use the `RMSprop` and `binary_crossentropy` for the loss since we're ending with a single sigmoid unit.

```{r}
 model %>% compile(loss = "binary_crossentropy",
                          optimizer = "rmsprop",
                          metrics = "accuracy")
```

### Data pre-processing
